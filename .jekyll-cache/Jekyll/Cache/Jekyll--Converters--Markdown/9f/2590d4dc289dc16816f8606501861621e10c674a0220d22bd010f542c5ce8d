I"
<h4 id="大数据">大数据</h4>
<p>上一节说到，大多的AI问题，会有很多个变量，这里深入的解释一下这个问题。<br />
比如说某个网站要做用户行为分析，从而指导网站建设的改进。通常而言如果没有行为分析，并不需要采集用户太多的数据。<br />
比如用户注册，最少只需要用户名、用户密码就够了。随后比如为了当用户过生日的时候，自动给用户发送一封贺卡（潜台词，我们可能需要给用户推送广告），我们再增加两项生日日期和邮箱地址。再下来国家规定网站注册必须实名制，我们可能又增加了用户姓名和身份证号码，可能还需要增加用户手机号码，用于同移动通信部门打通，验证用户实名制的真实性。这样一共是七个数据字段（仅为示例，事实比本例肯定要复杂很多倍）。<br />
随后通过网站的运营，用户数不断增加，到了某一天，网站的技术人员发现，数据量太大了，并发也太高了，一组服务器已经无法负担网站的运营。最重要其中的基础数据库也变得太大，系统无法容纳，需要分库、集群的新技术，才能保证网站的运营。<br />
从网站的运维来讲，这的确是一项重要的技术改进。但从“机器学习”的角度看，这些数据量的变化，并没有什么不同，可能在算法上，也不需要有太大的改变。所以严格上讲，这样的数据管理，还不能叫大数据。<br />
我们继续向下看，为了进行用户行为的分析。我们还要增加很多用户数据的采集点。比如用户每次访问网页的IP地址、用户的点击习惯、每个页面停留的时间、在页面上习惯点哪些位置的链接、操作上有什么习惯、用户的设备是什么型号、用户每次上网在什么时间段，这样需要关注的数据，我们还能列出很多。甚至可能会付费去第三方的公司购买很多其它的信息，比如我的用户还喜欢在什么网站停留，停留在其它网站的时候关注了什么内容，最近购买了什么东西等等信息。<br />
这些信息有下面几个特点：</p>
<ul>
  <li>并不是简单的信息表长度增长了，而是每行信息关注的维度大大的增加了，也就是信息表变宽了。</li>
  <li>随着维度的增加，总体的数据量可能会激增，很少的用户数，就需要利用到集群、并发等多种处理方式来解决压力问题。</li>
  <li>这些增加的维度，由于数量的增加，基本只能考虑用机器学习的方式，由计算机自动完成处理，分析其中的规律。</li>
</ul>
:ET