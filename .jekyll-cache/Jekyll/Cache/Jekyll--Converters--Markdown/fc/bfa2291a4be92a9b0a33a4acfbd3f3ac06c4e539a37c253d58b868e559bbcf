I"8M<h4 id="无监督学习">无监督学习</h4>
<p>前面已经说过了无监督学习的概念。无监督学习在实际的工作中应用还是比较多见的。<br />
从典型的应用上说，监督学习比较多用在“分类”上，利用给定的数据，做出一个决策，这个决策在有限的给定可能性中选择其中一种。各类识别、自动驾驶等都属于这一类。<br />
无监督学习则是“聚类”，算法自行寻找输入数据集的规律，并把它们按照规律分别组合，同样特征的放到一个类群。像自然语言理解、推荐算法、数据画像等，都属于这类（实际实现中还是比较多用半监督学习，但最早概念的导入还是属于无监督学习）。<br />
无监督学习的确是没有人工的标注，但所有输入的数据都必须保持原有的、必然存在的内在规律。为了保持这些规律或者挑选典型的规律，经常还是需要一些人力。<br />
介于两者之间的还有半监督学习，比如一半数据有标注，一半数据无标注。通过已标注数据分类，然后将无标注数据“聚类”到已知类型中去。从实现原理上或者组合了两种算法，或者实际上更倾向于监督学习，这里就不单独拿出来说了。<br />
前面看过了不少监督学习的例子，但还没有展示过无监督学习。今天就来剖析一个。</p>

<h4 id="单词向量化vector-representations-of-words">单词向量化（Vector Representations of Words）</h4>
<p>单词向量化是比较典型的无监督学习。这个概念的本意是这样：在自然语言处理（NLP）中，理解单词的含义是重要的一部分工作。因为我们说过，机器学习的本质是数学运算，解方程。此外单词的长度都不一致，根据归一化的原则，首先要做的事情就是把单词数字化成为统一的维度和数量级，就是每个单词用一个数字代替。几十年前的电报编码其实就是这个意思，一般常用的单词会用比较短的数字，这样数字化之后的长度更短，常用单词因为靠前，被检索的速度也会快。<br />
但是这样也带来一个大问题，就是单词原本是有一些内部隐藏含义的，比如man/woman。明显有些相关性的单词，数字化之后假设一个是56，一个是34，其中内部的含义就完全丢失了。cat / dog /animal这样的单词也是同样的，这丢失掉的信息对于NLP来讲，实际是很重要的部分。<br />
因此单词向量化的解决方法就是，把所有的单词嵌入到(embeding)到一个连续的向量空间中去。词义相近或者单词有潜在关联的单词，在向量空间中两个单词之间的距离就近。这个距离也可以作为衡量两个单词相似程度的标准。由此，单词向量化，也称为“word embedding”。<br />
因为单词向量化的工作是如此重要，TensorFlow官方提供了从低到高一整套示范或工具。</p>
<ul>
  <li>首先是一个简单的示范实现word2vec_basic.py，我们本篇主要看这个例子。</li>
  <li>因为word2vec工作非常耗时，官方又提供了一个升级版本word2vec_optimized.py，这个版本用c++重写了耗时的代码，作为TensorFlow的外部模块来使用，提供了较为正式的功能。因为这个代码在机器学习上并没有太多新概念。而又较多的使用了c++开发python外围模块的技术，更多的用于外围model的编写示例，所以这里只做一个简单介绍就跳过。有兴趣的朋友可以自己研究。</li>
  <li>官方提供了一个正式的命令行工具word2vec,可以使用pip安装，用于正式的一些单词向量化的工作。因为在NLP项目中，单词向量化通常都是第一步的工作，为NLP后续工作提供数据预处理。有了这个工具，很多工作就可以直接开始，不再另行编程。</li>
</ul>

<h4 id="基本原理">基本原理</h4>
<p>几乎所有实现单词向量化的算法都依赖于<a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis">分布式假设</a>，其核心思想为假定出现于相同上下文情景中的词汇都有相类似的语义。这个概念可能有些含糊，举个例子“我吃了个苹果”是一句话，另外一句话是“我吃了个香蕉”。作为非监督学习，这两句话不会做任何标注，但是经过训练的模型应当能理解“苹果”跟“香蕉”这两个词具有高度相似性，换言之，这两个词在向量空间中，应当具有很接近的距离。<br />
为了用算法实现这个概念，通常有两种方法：计数法和推理法。<br />
计数法：在大型语料库中对某词汇及其临近词汇进行统计计数，记下多种指标比如出现频率等，然后再根据这些量把所有单词映射到向量空间中去。<br />
推理法：也叫预测法，首先假设已经存在一个向量空间，利用这个空间中已经有的数据，通过某词汇的临近词汇，对词汇本身进行预测，对错误的预测在向量空间中调整其位置。<br />
其实这两种方法经常是结合使用的。<br />
在word2vec实例中，使用了基于极大似然法的概率化语言模型对连续单词进行关联性预测。极大似然法的资料可以参考最下面的参考链接部分，有一组公式用于实现这个算法。<br />
随后我们有了任何一个单词之后，根据单词上下文，上下文的定义在程序中是可以设置的，我们采用单词左边1个及右边1个单词作为上下文。举例说有一句话：<br />
<code class="language-plaintext highlighter-rouge">the quick brown fox jumped over the lazy dog</code><br />
以上下文相关的方式对每个单词进行分组可以得到：<br />
<code class="language-plaintext highlighter-rouge">([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...</code><br />
然后我们就可以利用the brown预测quick，用quick fox预测brown。这种预测方式也叫连续词袋模型（CBOW）。还有一种方式是反过来，同上例，比如我们用quick预测the brown，这样叫：Skip-Gram模型。<br />
从时间复杂性上说，CBOW算法适合较小的数据集，但准确度更高（用多个单词预测1个单词），Skip-Gram则适合较大数据集（用1个单词预测多个单词）。</p>

<h4 id="源码">源码</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/usr/bin/env python
# -*- coding=UTF-8 -*-
</span>
<span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
</span><span class="s">"""Basic word2vec example."""</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">tempfile</span> <span class="kn">import</span> <span class="n">gettempdir</span>
<span class="kn">import</span> <span class="nn">zipfile</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="n">urllib</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin
</span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Step 1: Download the data.
</span><span class="n">url</span> <span class="o">=</span> <span class="s">'http://mattmahoney.net/dc/'</span>

<span class="c1"># 从上面的URL下载给定的语料库
# 为了提高速度，这里手工下载后屏蔽了本函数，防止每次运行都重复下载速度太慢
# pylint: disable=redefined-outer-name
</span><span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
  <span class="s">"""Download a file if not present, and make sure it's the right size."""</span>
  <span class="n">local_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">gettempdir</span><span class="p">(),</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">local_filename</span><span class="p">):</span>
    <span class="n">local_filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span> <span class="o">+</span> <span class="n">filename</span><span class="p">,</span>
                                                   <span class="n">local_filename</span><span class="p">)</span>
  <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">stat</span><span class="p">(</span><span class="n">local_filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">statinfo</span><span class="p">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Found and verified'</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">statinfo</span><span class="p">.</span><span class="n">st_size</span><span class="p">)</span>
    <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">'Failed to verify '</span> <span class="o">+</span> <span class="n">local_filename</span> <span class="o">+</span>
                    <span class="s">'. Can you get to it with a browser?'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">local_filename</span>

<span class="c1">#单词数据包实际下载路径：http://mattmahoney.net/dc/text8.zip
#在这里下载后放到当前目录，所以下面filename做了修改，并且不再调用maybe_download函数
#filename = maybe_download('text8.zip', 31344016)
</span><span class="n">filename</span> <span class="o">=</span> <span class="s">"./text8.zip"</span>

<span class="c1">#从zip包中第一个文件读取所有的数据（实际只有一个文本文件），
#所有的数据只有词，以空格分割，没有标点符号。
#单词之间有语序关系，意思是某文章去掉标点符号之后，每句话中单词的语序仍然存在。
#为了加深印象，可以解压缩text8.zip包，然后显示文本文件看一下，
#文件很大，建议使用more只查看一部分
# Read the data into a list of strings.
</span><span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="s">"""Extract the first file enclosed in a zip file as a list of words."""</span>
  <span class="k">with</span> <span class="n">zipfile</span><span class="p">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">namelist</span><span class="p">()[</span><span class="mi">0</span><span class="p">])).</span><span class="n">split</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">data</span>

<span class="c1">#读取所有单词到字符串数组
</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Data size'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))</span>

<span class="c1">#整体数据，按照这个下载包是17005207个单词，下面50000是为了演示速度，限制了有效词数
# Step 2: Build the dictionary and replace rare words with UNK token.
</span><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">n_words</span><span class="p">):</span>
  <span class="s">"""Process raw inputs into a dataset."""</span>
  <span class="n">count</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'UNK'</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
  <span class="c1">#按照相同词统计数，进行排序，常用词在前面,最前面当然是UNK
</span>  <span class="c1">#其次是the/of/and/one，排序靠前的5个单词后面会显示出来...
</span>  <span class="n">count</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="p">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">).</span><span class="n">most_common</span><span class="p">(</span><span class="n">n_words</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
    <span class="c1">#计数增加，用于给每个词编一个唯一数字代码
</span>    <span class="c1">#UNK是第0个，编码是0，因为加入第一个UNK的时候，dictionary是空，所以len是0。
</span>    <span class="c1">#输出看的时候，因为是以单词顺序列出来，所以看着顺序混乱，
</span>    <span class="c1">#实际看反查表因为数字在前，看起来会更明显。
</span>    <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span> 
  <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="c1">#data最终是数字化之后的words，也就是数字化之后的原文，
</span>  <span class="c1">#其中按照原文顺序，每个元素，是该单词的数字编码
</span>  <span class="c1">#数字编码是从dictionary中查表找到的，也就是本函数前面数字化单词的过程得到的
</span>  <span class="n">unk_count</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># dictionary['UNK']
</span>      <span class="n">unk_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
  <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">unk_count</span>
  <span class="c1">#这个逆转字典是从数字到单词来对应，双向查表用的
</span>  <span class="n">reversed_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dictionary</span><span class="p">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dictionary</span><span class="p">.</span><span class="n">keys</span><span class="p">()))</span>
  <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reversed_dictionary</span>

<span class="c1"># Filling 4 global variables:
# data - list of codes (integers from 0 to vocabulary_size-1).
#   This is the original text but words are replaced by their codes
# count - map of words(strings) to count of occurrences
# dictionary - map of words(strings) to their codes(integers)
# reverse_dictionary - maps codes(integers) to words(strings)
#使用build_dataset函数填充4个全局变量，
#这些全局变量的内容刚才在函数注释中我们都介绍过了
#也可以参考上面官方原本的英文注释
</span><span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span>
                                                            <span class="n">vocabulary_size</span><span class="p">)</span>
<span class="c1">#做完了上面的数字化，原文其实就没用了，这里删除以节省内存
</span><span class="k">del</span> <span class="n">vocabulary</span>  <span class="c1"># Hint to reduce memory.
#count表上面说了，是统计出现次数，这里列出最常出现的5个单词
</span><span class="k">print</span><span class="p">(</span><span class="s">'Most common words (+UNK)'</span><span class="p">,</span> <span class="n">count</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="c1">#数字化后的前10个单词及查表得出的原文,
#注意后面的逆向表查表部分是python特有的语法，其它语言中不多见
</span><span class="k">print</span><span class="p">(</span><span class="s">'Sample data'</span><span class="p">,</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]])</span>

<span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Step 3: Function to generate a training batch for the skip-gram model.
</span><span class="k">def</span> <span class="nf">generate_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">):</span>
  <span class="k">global</span> <span class="n">data_index</span>
  <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
  <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># [ skip_window target skip_window ]
</span>  <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">data_index</span> <span class="o">+</span> <span class="n">span</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="nb">buffer</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">:</span><span class="n">data_index</span> <span class="o">+</span> <span class="n">span</span><span class="p">])</span>
  <span class="n">data_index</span> <span class="o">+=</span> <span class="n">span</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">num_skips</span><span class="p">):</span>
    <span class="n">context_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">skip_window</span><span class="p">]</span>
    <span class="n">words_to_use</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">context_word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words_to_use</span><span class="p">):</span>
      <span class="n">batch</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
      <span class="n">labels</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">context_word</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
      <span class="nb">buffer</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">span</span><span class="p">])</span>
      <span class="n">data_index</span> <span class="o">=</span> <span class="n">span</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="nb">buffer</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
      <span class="n">data_index</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="c1"># Backtrack a little bit to avoid skipping words in the end of a batch
</span>  <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">span</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span>

<span class="c1">#生成一批用于学习的数据集，这里首先生成一批很小的量
#然后在下面显示出来，用于人为观察生成的数据集是否合理
</span><span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_skips</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">skip_window</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span>
        <span class="s">'-&gt;'</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Step 4: Build and train a skip-gram model.
#这里定义的常量，才是真正学习的时候生成数据集的尺寸等参数
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># Dimension of the embedding vector.
# 左右各考虑1个单词
</span><span class="n">skip_window</span> <span class="o">=</span> <span class="mi">1</span>       <span class="c1"># How many words to consider left and right.
# 本窗口完成跳2个单词取样下一个窗口
</span><span class="n">num_skips</span> <span class="o">=</span> <span class="mi">2</span>         <span class="c1"># How many times to reuse an input to generate a label.
</span><span class="n">num_sampled</span> <span class="o">=</span> <span class="mi">64</span>      <span class="c1"># Number of negative examples to sample.
</span>
<span class="c1"># We pick a random validation set to sample nearest neighbors. Here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent. These 3 variables are used only for
# displaying model accuracy, they don't affect calculation.
</span><span class="n">valid_size</span> <span class="o">=</span> <span class="mi">16</span>     <span class="c1"># Random set of words to evaluate similarity on.
</span><span class="n">valid_window</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Only pick dev samples in the head of the distribution.
</span><span class="n">valid_examples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">valid_window</span><span class="p">,</span> <span class="n">valid_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>


<span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="k">with</span> <span class="n">graph</span><span class="p">.</span><span class="n">as_default</span><span class="p">():</span>

  <span class="c1"># Input data.
</span>  <span class="n">train_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
  <span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="n">valid_examples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>

  <span class="c1"># Ops and variables pinned to the CPU because of missing GPU implementation
</span>  <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'/cpu:0'</span><span class="p">):</span>
    <span class="c1"># Look up embeddings for inputs.
</span>    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">train_inputs</span><span class="p">)</span>

    <span class="c1"># Construct the variables for the NCE loss
</span>    <span class="n">nce_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span>
                            <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)))</span>
    <span class="n">nce_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">vocabulary_size</span><span class="p">]))</span>

  <span class="c1"># Compute the average NCE loss for the batch.
</span>  <span class="c1"># tf.nce_loss automatically draws a new sample of the negative labels each
</span>  <span class="c1"># time we evaluate the loss.
</span>  <span class="c1"># Explanation of the meaning of NCE loss:
</span>  <span class="c1">#   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
</span>  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span>
      <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">nce_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">nce_weights</span><span class="p">,</span>
                     <span class="n">biases</span><span class="o">=</span><span class="n">nce_biases</span><span class="p">,</span>
                     <span class="n">labels</span><span class="o">=</span><span class="n">train_labels</span><span class="p">,</span>
                     <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span>
                     <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_sampled</span><span class="p">,</span>
                     <span class="n">num_classes</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">))</span>

  <span class="c1"># Construct the SGD optimizer using a learning rate of 1.0.
</span>  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

  <span class="c1"># Compute the cosine similarity between minibatch examples and all embeddings.
</span>  <span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
  <span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">/</span> <span class="n">norm</span>
  <span class="n">valid_embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
      <span class="n">normalized_embeddings</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">)</span>
  <span class="n">similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span>
      <span class="n">valid_embeddings</span><span class="p">,</span> <span class="n">normalized_embeddings</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># Add variable initializer.
</span>  <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="c1"># Step 5: Begin training.
</span><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">100001</span>

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
  <span class="c1"># We must initialize all variables before we use them.
</span>  <span class="n">init</span><span class="p">.</span><span class="n">run</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Initialized'</span><span class="p">)</span>

  <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">batch_inputs</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">)</span>
    <span class="c1">#可以在tensorflow运算过程中逐批次喂入的数据集是由tf.placeholder定义的，
</span>    <span class="c1">#这里把所有要喂入的数据先包装成dict
</span>    <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">train_inputs</span><span class="p">:</span> <span class="n">batch_inputs</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">:</span> <span class="n">batch_labels</span><span class="p">}</span>

    <span class="c1"># We perform one update step by evaluating the optimizer op (including it
</span>    <span class="c1"># in the list of returned values for session.run()
</span>    <span class="c1">#运行，并逐批次喂入数据
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">loss_val</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
    <span class="n">average_loss</span> <span class="o">+=</span> <span class="n">loss_val</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">average_loss</span> <span class="o">/=</span> <span class="mi">2000</span>
      <span class="c1"># The average loss is an estimate of the loss over the last 2000 batches.
</span>      <span class="c1">#这里显示的是每2000批次平均出来的代价函数返回值
</span>      <span class="k">print</span><span class="p">(</span><span class="s">'Average loss at step '</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="s">': '</span><span class="p">,</span> <span class="n">average_loss</span><span class="p">)</span>
      <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Note that this is expensive (~20% slowdown if computed every 500 steps)
</span>    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">sim</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">valid_size</span><span class="p">):</span>
        <span class="n">valid_word</span> <span class="o">=</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">valid_examples</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># number of nearest neighbors
</span>        <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">sim</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]).</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">log_str</span> <span class="o">=</span> <span class="s">'Nearest to %s:'</span> <span class="o">%</span> <span class="n">valid_word</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">top_k</span><span class="p">):</span>
          <span class="n">close_word</span> <span class="o">=</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">nearest</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
          <span class="n">log_str</span> <span class="o">=</span> <span class="s">'%s %s,'</span> <span class="o">%</span> <span class="p">(</span><span class="n">log_str</span><span class="p">,</span> <span class="n">close_word</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">log_str</span><span class="p">)</span>
  <span class="n">final_embeddings</span> <span class="o">=</span> <span class="n">normalized_embeddings</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># Step 6: Visualize the embeddings.
</span>

<span class="c1"># pylint: disable=missing-docstring
# Function to draw visualization of distance between embeddings.
</span><span class="k">def</span> <span class="nf">plot_with_labels</span><span class="p">(</span><span class="n">low_dim_embs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">low_dim_embs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s">'More labels than embeddings'</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>  <span class="c1"># in inches
</span>  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">low_dim_embs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span>
                 <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                 <span class="n">textcoords</span><span class="o">=</span><span class="s">'offset points'</span><span class="p">,</span>
                 <span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">,</span>
                 <span class="n">va</span><span class="o">=</span><span class="s">'bottom'</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
  <span class="c1"># pylint: disable=g-import-not-at-top
</span>  <span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
  <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

  <span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exact'</span><span class="p">)</span>
  <span class="n">plot_only</span> <span class="o">=</span> <span class="mi">500</span>
  <span class="n">low_dim_embs</span> <span class="o">=</span> <span class="n">tsne</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">final_embeddings</span><span class="p">[:</span><span class="n">plot_only</span><span class="p">,</span> <span class="p">:])</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">plot_only</span><span class="p">)]</span>
  <span class="n">plot_with_labels</span><span class="p">(</span><span class="n">low_dim_embs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="s">'./tsne.png'</span><span class="p">)</span>

<span class="k">except</span> <span class="nb">ImportError</span> <span class="k">as</span> <span class="n">ex</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'Please install sklearn, matplotlib, and scipy to show embeddings.'</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>

</code></pre></div></div>
<p>源码没有使用从main()开始的函数式编程风格，较多的使用了过程式语言的方式。一块功能定义一个函数，然后接着就在python的全局开始初始化和调用刚才的函数，随后接着是下一个函数和相应的调用。<br />
除了以前见过的部分，源码中都做了比较多的注释。下面再对一些重点部分做一个讲解。<br />
讲解之前为了理解方便，这里先把语料库摘个开头贴一下：</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic institutions anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in stoic zeno of citium according to kropotkin zeno repudiated the omnipotence of the state its intervention and regimentation and proclaimed the sovereignty of the moral law of the individual the anabaptists of one six th century europe are sometimes considered to be religious forerunners of modern anarchism bertrand russell in his history of western philosophy writes that the anabaptists repudiated all law since they held that the good man will be guided at every moment by the holy spirit from
</code></pre></div></div>
<p>语料库是一个连续的文本文件，其中每个单词之间用一个空格隔开，没有标点符号、没有换行符等控制字符（所以上面的摘录，在终端中看是很多行，在这里显示为1行）。<br />
参考官方的讲解，我们这里也把程序分成6个部分：</p>
<ol>
  <li>
    <p>检测本地如果没有语料库，则去网上下载，下载路径是：<code class="language-plaintext highlighter-rouge">http://mattmahoney.net/dc/text8.zip</code>。<br />
同以前的例子相同，因为这个下载包压缩后30多M,我手工下载了语料库，简单的修改了程序，直接从当前目录打开<code class="language-plaintext highlighter-rouge">text8.zip</code>文件，以便节省时间。<br />
比后面进阶示例好的地方是，本例中使用了zipfile包来直接读取压缩包中的语料库，不用再解压出来，否则可是100多M的一个文本文件。<br />
单词会读到vocabulary数组，每个单词占用一个数组元素。数组的顺序就是原来在语料库中单词出现的顺序。</p>
  </li>
  <li>
    <p>进行基本的数据整理， 示例起见，这里只使用前面的50000个单词对模型进行训练。在训练集中，统计单词的出现频率，并根据频率生成字典dictionary。字典中频率高的靠前放，在字典中的排名将是这个单词的编号。出现很少的单词替换为“UNK”(因为这种出现非常少的单词没有参考对象，无法进行训练和预测。因此干脆用UNK代替，等于是剔除)。UNK在字典中是第1个，编号是0；后面则按照出现频率排序。程序开始运行时会显示前5个高频词，应当如下：<br />
<code class="language-plaintext highlighter-rouge">Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]</code><br />
也说明the的编号为1，of是2，and是3，one是4。<br />
随后使用这个字典，对整个语料库进行数字化，数字化的结果存在data之中，其中稀有词UNK已经被去掉。完成后将是类似这样：<br />
<code class="language-plaintext highlighter-rouge">5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156</code>,这组数字代表原文中的前10个单词：<br />
<code class="language-plaintext highlighter-rouge">'anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against'</code><br />
最后，考虑到单词数字化之后，还会需要被逆转成单词，因此又生成了reversed_dictionary字典，其中键值是数字，值则是单词，用于逆向检索。</p>
  </li>
  <li>
    <p>定义了一个函数，用于生成训练用的数据集。根据训练的特点，训练集是批次生成的。定义完这个函数，使用了一个很小的量（程序中是8）实验生成了一下。这里重点需要理解函数的3个参数：batch_size是每批次生成的单词量；num_skips代表单词窗口移动时跳过的单词数；skip_window是当前单词左右几个单词作为本单词的上下文。前面讲过了，Skip_gram算法是用当前单词，在训练好的模型中预测这个上下文。</p>
  </li>
  <li>构建用于训练的skip-gram模型，重点2个：
    <ul>
      <li>没有使用我们熟悉的softmax分类，而是用了nce_loss，同样具体公式在网上查（其实官方word2vec课程中就有，这里略去了）。主要原因是softmax对于一个巨大的分类系统工作非常缓慢，此外nce_loss算法的数学特征使得预测命中的词给出高概率（极大似然法，公式最终结果是可能性概率），给没有命中的词（噪音词）给出低概率，从而达成抑制噪音的目的。</li>
      <li>tf.nn.embedding_lookup是一个新接触到的函数，为了便于理解，我们后面给一个小例子。其余部分虽然模型不常见，但基本都应当能读懂。</li>
    </ul>
  </li>
  <li>训练模型，最终向量化的结果，从TensorFlow输出保存到python变量final_embeddings里面。中间每10000步会列出16个单词及其相似单词，这个功能可以清晰的看到相似度正确率的提高。当然作为一个示例程序，离正式的应用还是有很大差距的。</li>
  <li>将得到的向量化结果，抽取前500个，绘制出来，输出为png图片。从图片上看，能够更形象的理解单词向量化的概念。</li>
</ol>

<p>还有一点要讲解的是，我们前面的例子一直强调归一化的重要性，在word2vec中，除了数字化，基本没有别的归一化动作。原因很多，最主要的是，在以前的例子中，我们更关注量的概念，拟合到比较接近的数值就算很好的结果。而对数字化之后的单词来讲，每个整数对应一个单词，不可能有小数，就算值相差1，也代表了完全不同的单词。因此在本例中没有传统转换成浮点数那种归一化操作。</p>

<h4 id="tfnnembedding_lookup的功能">tf.nn.embedding_lookup的功能</h4>
<p>来看个小例子：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/usr/bin/env/python
# coding=utf-8
</span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>

<span class="c1">#定义一个5x5对角矩阵，样式可以看运行结果第一个输出 
</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">))</span>
<span class="c1">#使用embedding_lookup检索矩阵，检索数据集是input_ids
</span><span class="n">input_embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">InteractiveSession</span><span class="p">()</span>
<span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">embedding</span><span class="p">.</span><span class="nb">eval</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_ids</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]}))</span>
</code></pre></div></div>
<p>执行结果：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>embedding <span class="o">=</span> <span class="o">[[</span>1 0 0 0 0]
             <span class="o">[</span>0 1 0 0 0]
             <span class="o">[</span>0 0 1 0 0]
             <span class="o">[</span>0 0 0 1 0]
             <span class="o">[</span>0 0 0 0 1]]
input_embedding <span class="o">=</span> <span class="o">[[</span>0 1 0 0 0]
                   <span class="o">[</span>0 0 1 0 0]
                   <span class="o">[</span>0 0 0 1 0]
                   <span class="o">[</span>1 0 0 0 0]
                   <span class="o">[</span>0 0 0 1 0]
                   <span class="o">[</span>0 0 1 0 0]
                   <span class="o">[</span>0 1 0 0 0]]
</code></pre></div></div>
<p>embedding_lookup的功能，就是根据input_ids中的id，寻找embedding中的对应行的元素，逐行结果组合在一起，成为一个新的矩阵返回。比如上面就是embedding第1、2、3、0、3、2、1行的结果，重新组合成一个7行的矩阵返回给input_embedding。</p>

<h4 id="进阶实现">进阶实现</h4>
<p><a href="https://github.com/tensorflow/models/tree/master/tutorials/embedding">进阶版本源码</a>是一个基本可以应用的实例，在项目页面的介绍中有使用办法，但在macOS中运行有些问题，这里做个说明。<br />
首先是编译，我是用的TensorFlow1.4.1版本没有这个方法<code class="language-plaintext highlighter-rouge">tf.sysconfig.get_compile_flags()</code>，无法得到正确的编译参数，最后只好写了一个脚本进行编译：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>

<span class="nv">TF_CFLAGS</span><span class="o">=</span><span class="s2">"-I/usr/local/lib/python2.7/site-packages/tensorflow/include"</span>
<span class="nv">TF_LFLAGS</span><span class="o">=</span><span class="s2">"-L/usr/local/lib/python2.7/site-packages/tensorflow"</span>

g++ <span class="nt">-std</span><span class="o">=</span>c++11 <span class="nt">-shared</span> word2vec_ops.cc word2vec_kernels.cc <span class="nt">-o</span> word2vec_ops.so <span class="nt">-fPIC</span> <span class="k">${</span><span class="nv">TF_CFLAGS</span><span class="k">}</span> <span class="k">${</span><span class="nv">TF_LFLAGS</span><span class="k">}</span> <span class="nt">-O2</span> <span class="nt">-D_GLIBCXX_USE_CXX11_ABI</span><span class="o">=</span>0 <span class="nt">-undefined</span> dynamic_lookup
</code></pre></div></div>
<p>方法就是人工找到INCLUDE和LIB路径，将路径设置为常量，在编译中直接给定。<br />
需要注意是在macOS上编译，必须使用<code class="language-plaintext highlighter-rouge">-undefined dynamic_lookup</code>，不然链接的时候会报错。<br />
编译之后得到的so文件，会在python程序中使用如下方法调用:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word2vec</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">load_op_library</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">__file__</span><span class="p">)),</span> <span class="s">'word2vec_ops.so'</span><span class="p">))</span>
<span class="p">...</span>
    <span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">words_per_epoch</span><span class="p">,</span> <span class="n">current_epoch</span><span class="p">,</span> <span class="n">total_words_processed</span><span class="p">,</span>
     <span class="n">examples</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">word2vec</span><span class="p">.</span><span class="n">skipgram_word2vec</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">opts</span><span class="p">.</span><span class="n">train_data</span><span class="p">,</span>
                                                    <span class="n">batch_size</span><span class="o">=</span><span class="n">opts</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span>
                                                    <span class="n">window_size</span><span class="o">=</span><span class="n">opts</span><span class="p">.</span><span class="n">window_size</span><span class="p">,</span>
                                                    <span class="n">min_count</span><span class="o">=</span><span class="n">opts</span><span class="p">.</span><span class="n">min_count</span><span class="p">,</span>
                                                    <span class="n">subsample</span><span class="o">=</span><span class="n">opts</span><span class="p">.</span><span class="n">subsample</span><span class="p">)</span>
</code></pre></div></div>

<p>数据文件的准备使用官方给出的命令没有问题：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl http://mattmahoney.net/dc/text8.zip <span class="o">&gt;</span> text8.zip
unzip text8.zip
curl https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip <span class="o">&gt;</span> source-archive.zip
unzip <span class="nt">-p</span> source-archive.zip  word2vec/trunk/questions-words.txt <span class="o">&gt;</span> questions-words.txt
<span class="nb">rm </span>text8.zip source-archive.zip
</code></pre></div></div>
<p>用于评估训练结果的问题集因为在google的服务器上，可能需要翻墙才能下载。</p>

<p>最后word2vec_optimized.py的执行结果如下：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">2018</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">16</span> <span class="mi">13</span><span class="p">:</span><span class="mi">17</span><span class="p">:</span><span class="mf">14.277603</span><span class="p">:</span> <span class="n">I</span> <span class="n">word2vec_kernels</span><span class="p">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">200</span><span class="p">]</span> <span class="n">Data</span> <span class="nb">file</span><span class="p">:</span> <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">andrew</span><span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">tensorFlow</span><span class="o">/</span><span class="n">word2vec</span><span class="o">/</span><span class="n">text8</span> <span class="n">contains</span> <span class="mi">100000000</span> <span class="nb">bytes</span><span class="p">,</span> <span class="mi">17005207</span> <span class="n">words</span><span class="p">,</span> <span class="mi">253854</span> <span class="n">unique</span> <span class="n">words</span><span class="p">,</span> <span class="mi">71290</span> <span class="n">unique</span> <span class="n">frequent</span> <span class="n">words</span><span class="p">.</span>
<span class="n">Data</span> <span class="nb">file</span><span class="p">:</span>  <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">andrew</span><span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">tensorFlow</span><span class="o">/</span><span class="n">word2vec</span><span class="o">/</span><span class="n">text8</span>
<span class="n">Vocab</span> <span class="n">size</span><span class="p">:</span>  <span class="mi">71290</span>  <span class="o">+</span> <span class="n">UNK</span>
<span class="n">Words</span> <span class="n">per</span> <span class="n">epoch</span><span class="p">:</span>  <span class="mi">17005207</span>
<span class="n">Eval</span> <span class="n">analogy</span> <span class="nb">file</span><span class="p">:</span>  <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">andrew</span><span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">tensorFlow</span><span class="o">/</span><span class="n">word2vec</span><span class="o">/</span><span class="n">questions</span><span class="o">-</span><span class="n">words</span><span class="p">.</span><span class="n">txt</span>
<span class="n">Questions</span><span class="p">:</span>  <span class="mi">17827</span>
<span class="n">Skipped</span><span class="p">:</span>  <span class="mi">1717</span>
<span class="n">Epoch</span>    <span class="mi">1</span> <span class="n">Step</span>   <span class="mi">150943</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.024</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">31527</span>
<span class="n">Eval</span> <span class="mi">1469</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span>  <span class="mf">8.2</span><span class="o">%</span>
<span class="n">Epoch</span>    <span class="mi">2</span> <span class="n">Step</span>   <span class="mi">301913</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.023</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">25120</span>
<span class="n">Eval</span> <span class="mi">2395</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">13.4</span><span class="o">%</span>
<span class="n">Epoch</span>    <span class="mi">3</span> <span class="n">Step</span>   <span class="mi">452887</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.021</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>     <span class="mi">8842</span>
<span class="n">Eval</span> <span class="mi">3014</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">16.9</span><span class="o">%</span>
<span class="n">Epoch</span>    <span class="mi">4</span> <span class="n">Step</span>   <span class="mi">603871</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.020</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>     <span class="mi">6615</span>
<span class="n">Eval</span> <span class="mi">3532</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">19.8</span><span class="o">%</span>
<span class="n">Epoch</span>    <span class="mi">5</span> <span class="n">Step</span>   <span class="mi">754815</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.019</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>     <span class="mi">3007</span>
<span class="n">Eval</span> <span class="mi">3994</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">22.4</span><span class="o">%</span>
<span class="n">Epoch</span>    <span class="mi">6</span> <span class="n">Step</span>   <span class="mi">905787</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.018</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">26590</span>
<span class="n">Eval</span> <span class="mi">4320</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">24.2</span><span class="o">%</span>
<span class="n">Epoch</span>    <span class="mi">7</span> <span class="n">Step</span>  <span class="mi">1056767</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.016</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">35439</span>
<span class="n">Eval</span> <span class="mi">4714</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">26.4</span><span class="o">%</span>
<span class="n">Epoch</span>    <span class="mi">8</span> <span class="n">Step</span>  <span class="mi">1207755</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.015</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>      <span class="mi">401</span>
<span class="n">Eval</span> <span class="mi">4965</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">27.9</span><span class="o">%</span>
<span class="n">Epoch</span>    <span class="mi">9</span> <span class="n">Step</span>  <span class="mi">1358735</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.014</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">36991</span>
<span class="n">Eval</span> <span class="mi">5276</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">29.6</span><span class="o">%</span>
<span class="n">Epoch</span>   <span class="mi">10</span> <span class="n">Step</span>  <span class="mi">1509744</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.013</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">25069</span>
<span class="n">Eval</span> <span class="mi">5415</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">30.4</span><span class="o">%</span>
<span class="n">Epoch</span>   <span class="mi">11</span> <span class="n">Step</span>  <span class="mi">1660729</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.011</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">28271</span>
<span class="n">Eval</span> <span class="mi">5649</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">31.7</span><span class="o">%</span>
<span class="n">Epoch</span>   <span class="mi">12</span> <span class="n">Step</span>  <span class="mi">1811667</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.010</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">29973</span>
<span class="n">Eval</span> <span class="mi">5880</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">33.0</span><span class="o">%</span>
<span class="n">Epoch</span>   <span class="mi">13</span> <span class="n">Step</span>  <span class="mi">1962606</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.009</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">10225</span>
<span class="n">Eval</span> <span class="mi">6015</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">33.7</span><span class="o">%</span>
<span class="n">Epoch</span>   <span class="mi">14</span> <span class="n">Step</span>  <span class="mi">2113546</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.008</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">21419</span>
<span class="n">Eval</span> <span class="mi">6270</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">35.2</span><span class="o">%</span>
<span class="n">Epoch</span>   <span class="mi">15</span> <span class="n">Step</span>  <span class="mi">2264489</span><span class="p">:</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.006</span> <span class="n">words</span><span class="o">/</span><span class="n">sec</span> <span class="o">=</span>    <span class="mi">27059</span>
<span class="n">Eval</span> <span class="mi">6434</span><span class="o">/</span><span class="mi">17827</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">36.1</span><span class="o">%</span>
</code></pre></div></div>
<p>程序看上去要复杂很多。主要目的是展示把耗时的操作、而TensorFlow中又没有实现的算法，用c++写成TensorFlow扩展包的形式来实现一个复杂的机器学习模型。所以这里不过多说源码，有兴趣的读者可以自行分析。<br />
最后看一下用于评估的问题库的格式：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">:</span> <span class="n">capital</span><span class="o">-</span><span class="n">common</span><span class="o">-</span><span class="n">countries</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Baghdad</span> <span class="n">Iraq</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Bangkok</span> <span class="n">Thailand</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Beijing</span> <span class="n">China</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Berlin</span> <span class="n">Germany</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Bern</span> <span class="n">Switzerland</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Cairo</span> <span class="n">Egypt</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Canberra</span> <span class="n">Australia</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Hanoi</span> <span class="n">Vietnam</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Havana</span> <span class="n">Cuba</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Helsinki</span> <span class="n">Finland</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Islamabad</span> <span class="n">Pakistan</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Kabul</span> <span class="n">Afghanistan</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">London</span> <span class="n">England</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Madrid</span> <span class="n">Spain</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Moscow</span> <span class="n">Russia</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Oslo</span> <span class="n">Norway</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Ottawa</span> <span class="n">Canada</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Paris</span> <span class="n">France</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Rome</span> <span class="n">Italy</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Stockholm</span> <span class="n">Sweden</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Tehran</span> <span class="n">Iran</span>
<span class="n">Athens</span> <span class="n">Greece</span> <span class="n">Tokyo</span> <span class="n">Japan</span>
<span class="n">Baghdad</span> <span class="n">Iraq</span> <span class="n">Bangkok</span> <span class="n">Thailand</span>
<span class="n">Baghdad</span> <span class="n">Iraq</span> <span class="n">Beijing</span> <span class="n">China</span>
<span class="n">Baghdad</span> <span class="n">Iraq</span> <span class="n">Berlin</span> <span class="n">Germany</span>
<span class="n">Baghdad</span> <span class="n">Iraq</span> <span class="n">Bern</span> <span class="n">Switzerland</span>
<span class="p">...</span>
</code></pre></div></div>
<p>其中使用冒号“:”开头的行是注释行，程序中会跳过。<br />
随后是<code class="language-plaintext highlighter-rouge">城市-首都 城市-首都</code>这样形式的关联对，4个词在一行。预测方法就是用前三个词，预测最后一个词，如果预测对了，则正确率+1。可见在训练语料库text8跟评估使用的问题集questions-words.txt完全不同、且没有任何关联性的两个数据集中，达到36.1%的预测正确率是多么不容易(另外这个示例也没有完成全部的训练，否则正确率还可以提高)。 
依赖这种特征，单词向量化也经常用于呼叫中心知识库的智能检索，实现智能回答机器人的一些实现中。</p>

<p>(待续…)</p>

<h4 id="引文及参考">引文及参考</h4>
<p><a href="http://www.tensorfly.cn/tfdoc/tutorials/word2vec.html">TensorFlow中文社区word2vec讲解</a><br />
<a href="https://www.jianshu.com/p/f682066f0586">图解word2vec</a><br />
<a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">极大似然法</a><br />
<a href="https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf">Dependency-Based Word Embeddings</a></p>

:ET