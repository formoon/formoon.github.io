I"<script src="https://cdn.bootcdn.net/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<p><img src="/attachments/201906/gradient/gradient-descent.jpeg" alt="" /><br />
前面一篇就是基础性的推导过程。从反馈的情况看，总体还是讲明白了。但是在导数的部分，仍有不少的存疑。<br />
其实在数学方面，我也是学渣。所以尽我所能，希望再次的补充能讲的明白。若有谬误，期盼指正。</p>
<h3 id="基础公式">基础公式</h3>
<p>所需基础公式抄录于下，不明白的请至<a href="http://blog.17study.com.cn/2019/06/21/basic-gradient-descent/">上篇</a>查看详解。</p>
<h4 id="假设函数">假设函数</h4>
<p>
$$
y' = h_θ(x) = \sum_{i=0}^nθ_ix_i
$$
</p>
<h4 id="均方差损失函数">均方差损失函数</h4>
<p>
$$
J(θ) = \frac1{2m}\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2
$$
</p>
<h4 id="梯度下降求解θ">梯度下降求解θ</h4>
<p>
$$
θ_j := θ_j - α\frac∂{∂θ_j}J(θ)
$$
摘出来上面公式步长α之后的部分：  
$$
\begin{align}
\frac∂{∂θ_j}J(θ) &amp; = \frac∂{∂θ_j}\frac1{2m}\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 \\
                &amp; = \frac1m\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^i
\end{align}
$$
</p>
<p>嗯，问题一般就是出在这里了，很多人尝试了化简，得不到上面的化简结果。</p>

<h3 id="导数公式">导数公式</h3>
<p>化简上面的式子，需要微积分导数的一些知识，我抄录用到的部分于此，以方便对照查看：</p>
<h4 id="导数">导数</h4>
<p>导数的目的是求得在给定点的切线方向，以保证梯度下降的下一步会向收敛方向（也即上面的损失函数最小化方向）迭代一个步长α。这个很多教程都讲过了，这里不再废话。<br />
<img src="/attachments/201906/gradient/gradient-descent-derivative.jpg" alt="" /><br />
(偷懒从网上搜了张图，侵删。图中的W实际是我们公式中的θ，J(W)就是我们讲的J(θ))</p>
<p>
首先公式\(\frac∂{∂θ_j}\)就是求导数的意思，别当做普通的分式，直接分子、分母把∂化简掉成为\(\frac1{θ_j}\)。当然大多数人不会这样做了，我只是见过这样的情况，说出来以防万一。</p>
<p>事实上，你把\(\frac∂{∂θ_j}\)换成常用的函数描述\(f(θ_j)\)可能更贴切。  
</p>
<h4 id="对函数的和求导法则">对函数的和求导法则</h4>
<p>为了描述起来方便，我们下面使用’符号来代表求导：</p>
<p>$$
(u + v)' = u' + v'
$$
在上面的公式中推广一下，Sigma求和不影响求导的传导，直接把Sigma符号提到前面就好：  
$$
(\sum_{i=1}^mu^{(i)})' = \sum_{i=1}^m(u^{(i)})' 
$$
</p>
<h4 id="对函数的积求导法则">对函数的积求导法则</h4>
<p>$$
(uv)' = u'v+uv'
$$
</p>
<h4 id="幂函数求导法则">幂函数求导法则</h4>
<p>
$$
(x^u)' = ux^{(u-1)}
$$
</p>
<h4 id="对常数求导">对常数求导</h4>
<p>这是我最爱的部分：</p>
<p>
$$
(C)' = 0
$$
</p>
<h4 id="链式法则">链式法则</h4>
<p>这是我最不喜欢的部分：<br />
假设我们希望对变量z求导，而变量z依赖变量y,变量y又依赖变量x。例如：</p>
<p>
$$
z = f(y) \\
y = g(x)
$$
也即:
$$
z = f(g(x))
$$
那么对z求导就构成了链式法则：  
$$
(z)' = (f(g(x)))'·（g(x))'
$$
注意最后面乘上内部依赖函数求导的过程，简直是反人类的天外来客，经常会忘。但我等遵循自然界规则的凡人又能如何，死记而已。  
</p>

<h3 id="推导">推导</h3>
<p>基本公式列完，开始推导过程：</p>
<p>
$$
\frac∂{∂θ_j}J(θ) = \frac∂{∂θ_j}\frac1{2m}\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 
$$
根据上面说的求和函数求导法则：  
$$
= \frac1{2m}\sum_{i=1}^m(\frac∂{∂θ_j}(h_θ(x^{(i)}) - y^{(i)})^2)
$$
别急着对幂求导，考虑对中间的损失函数的依赖，实际要先处理链式法则：  
$$
= \frac1{2m}\sum_{i=1}^m(\frac∂{∂θ_j}(h_θ(x^{(i)}) - y^{(i)})^2)·(\frac∂{∂θ_j}(h_θ(x^{(i)}) - y^{(i)})
$$
现在方程式前面的部分可以幂求导了，后面的部分把假设函数先展开：
$$
= \frac1{2m}\sum_{i=1}^m2·(h_θ(x^{(i)}) - y^{(i)}))·(\frac∂{∂θ_i}(\sum_{i=0}^nθ_ix_i - y^{(i)}))
$$
因为展开的假设函数中使用i代表第i个权重，所以前面的求导也换成了\(θ_i\)，不是指第i个批次的样本数据。这里原来没有打算展开讲，所以使用的符号名称有点容易混，但概念清楚的话不应当闹误会。  
</p>
<p>继续，式子前半部分的2跟1/2会抵消掉，这是前篇做均方差时候乘1/2的目的；后面的Sigma求导继续使用求和函数求导法则展开：</p>
<p>
$$
= \frac1{m}\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)}))·(\sum_{i=0}^n\frac∂{∂θ_i}θ_ix_i - \frac∂{∂θ_i}y^{(i)})
$$
前半部分的化简已经完成，简单起见，我们只把后面部分摘出来：  
$$
\sum_{i=0}^n\frac∂{∂θ_i}θ_ix_i - \frac∂{∂θ_i}y^{(i)}\\
= \frac∂{∂θ_i}(θ_0x_0+θ_1x_1+...+θ_ix_i+...+θ_nx_n) - \frac∂{∂θ_i}y^{(i)}
$$
根据求和函数求导法则展开，等于对其中每一项求导。而我们在对\(θ_i\)进行求导的时候，其余各项对我们来说，实际上就是一个常数，它们在求导这一刻是固定不能变的。嗯嗯，记得上一篇最后的提醒吗？θ在每个循环内固定不变，在计算完所有的θ之后，才一次代入，并在下个循环内保持不变。  
</p>
<p>
而对常数求导，刚才说过了，那是我的最爱，因为结果是0。还有我们抄了好几行的\(y^{(i)}\)求导，我忍得好辛苦，因为那也是样本集给出的常数，所以结果也是0：  
$$
= 0 + 0 + ... + \frac∂{∂θ_i}θ_ix_i + ... + 0 - 0
$$
现在需要对乘积函数求导展开了：  
$$
= \frac∂{∂θ_i}θ_i·x_i + θ_i·\frac∂{∂θ_i}x_i
$$
你看，这世界不总是那么残酷的，后面的\(x_i\)又双叒叕是一个常量，所以求导之后乘上\(θ_i\)仍然是0。  
前面对\(θ_i\)的求导结果是1，原因很简单，你可以把\(θ_i\)看做1次幂。  
$$
\begin{align}
&amp; = \frac∂{∂θ_i}(θ_i)^1·x_i + 0 \\
&amp; = 1·θ_i^{(1-1)}·x_i \\
&amp; = 1·1·x_i \\
&amp; = x_i \\
\end{align}
$$
只是瞬间，这个世界就清净了。原来对假设函数求导的最终结果，不过是\(θ_i\)的系数\(x_i\)。  
</p>
<p>前面我们两次把等式的局部摘出来化简，现在是把它们组合回去的时候了：</p>
<p>
$$
\begin{align}
θ_j &amp; = θ_j - α\frac∂{∂θ_j}J(θ) \\
&amp; = α\frac∂{∂θ_j}\frac1{2m}\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 \\
&amp; = α\frac1m\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^i
\end{align}
$$
</p>
<p>希望不用再写补充的补充的补充了吧。</p>
:ET