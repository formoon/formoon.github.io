I"<p><img src="https://raw.githubusercontent.com/formoon/formoon.github.io/master/attachments/201904/tensorFlow2/tf-logo-card-2.png" alt="" /></p>
<h4 id="引言">引言</h4>
<p>原来引用过一个段子，这里还要再引用一次。是关于苹果的。大意是，苹果发布了新的开发语言Swift，有非常多优秀的特征，于是很多时髦的程序员入坑学习。不料，经过一段头脑体操一般的勤学苦练，发现使用Swift做开发，不仅要学习Swift，还要学习Swift2、Swift3、Swift4…<br />
后来我发现，这个段子很有普遍性，并非仅仅苹果如此，今天的TensorFlow 2.0也有点这样的趋势。以至于我不得不专门写一个课程的续集，来面对使用新版本软件开始机器学习的读者。<br />
事实上大多具有革命性的公司都是这样，一方面带来令人兴奋的新特征，另一方面则是高企不落的学习成本。</p>

<p><a href="http://blog.17study.com.cn/2018/01/08/tensorFlow-series-1/">《从锅炉工到AI专家》</a>一文中，已经对机器学习的基本概念做了很详细的介绍。所以在这里我们就省掉闲言絮语，直接从TensorFlow2.0讲起。<br />
当然即便没有看过这个系列，假设你对TensorFlow 1.x很熟悉，也可以直接通过阅读本文，了解从TensorFlow 1.x迁移至2.x的知识。<br />
如果你不了解机器学习的概念，试图通过直接学习TensorFlow 2.0开始AI开发，那可能比较困难。TensorFlow只是工具。没有技能，只凭工具，你恐怕无法踏上旅程。</p>

<h4 id="安装">安装</h4>
<p>截至本文写作的时候，TensorFlow 2.0尚未正式的发布。pip仓库中仍然是1.13稳定版。所以如果想开始TensorFlow 2.0的学习，需要指定版本号来安装。此外由于Python2系列将于2020年元月停止官方维护，本文的示例使用Python3的代码来演示：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>pip3 <span class="nb">install </span><span class="nv">tensorflow</span><span class="o">==</span>2.0.0-alpha0
</code></pre></div></div>
<p>(注: 上面$是Mac/Linux的提示符，假如用Windows,你看到的提示符应当类似C:\Users\Administrator&gt;这样子。)</p>

<p>如果希望使用GPU计算，安装的预先准备会麻烦一些，请参考这篇文档：<a href="https://www.tensorflow.org/install/gpu">https://www.tensorflow.org/install/gpu</a>。主要是安装CUDA/cuDNN等计算平台的工具包。其中CUDA可以使用安装程序直接安装。cuDNN是压缩包，如果不打算自己编译TensorFlow的话，放置到CUDA相同目录会比较省事。<br />
这里提醒一下，除非自己编译TensorFlow，否则一定使用CUDA 10.0的版本，低了、高了都不成，因为官方的2.0.0-alpha0使用了CUDA 10.0的版本编译。<br />
此外TensorFlow的安装请使用如下命令：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>pip3 <span class="nb">install </span>tensorflow-gpu<span class="o">==</span>2.0.0-alpha0
</code></pre></div></div>

<p>安装完成后，可以在Python的交互模式，来确认TensorFlow正常工作：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>python3
Python 3.7.2 <span class="o">(</span>default, Feb 13 2019, 13:59:29<span class="o">)</span> 
<span class="o">[</span>Clang 10.0.0 <span class="o">(</span>clang-1000.11.45.5<span class="o">)]</span> on darwin
Type <span class="s2">"help"</span>, <span class="s2">"copyright"</span>, <span class="s2">"credits"</span> or <span class="s2">"license"</span> <span class="k">for </span>more information.
<span class="o">&gt;&gt;&gt;</span> import tensorflow as tf
<span class="o">&gt;&gt;&gt;</span> tf.__version__
<span class="s1">'2.0.0-alpha0'</span>
<span class="o">&gt;&gt;&gt;</span> 
</code></pre></div></div>
<p>本文中还会用到几个第三方的python扩展库，也是在机器学习中非常常用的，建议一起安装：</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>pip3 <span class="nb">install </span>numpy matplotlib pillow pandas seaborn sklearn
</code></pre></div></div>

<h4 id="第一个例子房价预测">第一个例子：房价预测</h4>
<p>本示例中的源码来自于<a href="http://blog.17study.com.cn/2018/01/08/tensorFlow-series-2/">《从锅炉工到AI专家》系列2</a>，使用了最简单的线性函数来做房价预测。原始TensorFlow 1.x/ Python 2.x代码如下：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/usr/bin/env python 
# -*- coding=UTF-8 -*-
</span>
<span class="c1">#本代码在mac电脑，python2.7环境测试通过
#第一行是mac/Linux系统脚本程序的标志，表示从环境参量中寻找python程序解释器来执行本脚本
#省去了每次在命令行使用 python &lt;脚本名&gt; 这样的执行方式
#第二行表示本脚本文本文件存盘使用的代码是utf-8,并且字符串使用的编码也是utf-8,
#在本源码中，这一点其实没有什么区别，但如果需要中文输出的时候，这一行就必须要加了。
</span>
<span class="c1">#引入TensorFlow库
</span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="c1">#引入数值计算库
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#使用 NumPy 生成假数据集x,代表房间的平米数，这里的取值范围是0-1的浮点数，
#原因请看正文中的说明，属于是“规范化”之后的数据
# 生成的数据共100个，式样是100行，每行1个数据
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1">#我们假设每平米0.5万元，基础费用0.7万，这个数值也是规范化之后的，仅供示例
#最终运行的结果，应当求出来0.5/0.7这两个值代表计算成功
#计算最终房价y，x和y一同当做我们的样本数据
# np.dot的意思就是向量x * 0.5
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span>
<span class="c1">#---------------------------------数据集准备完成
#以下使用TensorFlow构建数学模型，在这个过程中，
#直到调用.run之前，实际上都是构造模型，而没有真正的运行。
#这跟上面的numpy库每一次都是真正执行是截然不同的区别
# 请参考正文，我们假定房价的公式为：y=a*x+b
</span>
<span class="c1">#tf.Variable是在TensorFlow中定义一个变量的意思
#我们这里简单起见，人为给a/b两个初始值，都是0.3，注意这也是相当于规范化之后的数值
</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="c1">#这是定义主要的数学模型，模型来自于上面的公式
#注意这里必须使用tf的公式，这样的公式才是模型
#上面使用np的是直接计算，而不是定义模型
# TensorFlow的函数名基本就是完整英文，你应当能读懂
</span><span class="n">y_value</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># 这里是代价函数，同我们文中所讲的唯一区别是用平方来取代求绝对值，
#目标都是为了得到一个正数值，功能完全相同，
#平方计算起来会更快更容易,这种方式也称为“方差“
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_value</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
<span class="c1"># TensorFlow内置的梯度下降算法，每步长0.5
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># 代价函数值最小化的时候，代表求得解
</span><span class="n">train</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1"># 初始化所有变量，也就是上面定义的a/b两个变量
</span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="c1">#启动图
</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span>
<span class="c1">#真正的执行初始化变量，还是老话，上面只是定义模型，并没有真正开始执行
</span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

<span class="c1">#重复梯度下降200次，每隔5次打印一次结果
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
    <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>	
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span> <span class="n">step</span><span class="p">,</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div>
<p>代码保留了原始的注释，希望如果概念已经没有问题的话，可以让你不用跑回原文去看详细讲解。<br />
程序使用numpy生成了一组样本集，样本集是使用线性函数生成的。随后使用TensorFlow学习这些样本，从而得到线性函数中未知的权重(Weight)和偏移(Bias)值。<br />
原文中已经说了，这个例子并没有什么实用价值，只是为了从基础开始讲解“机器学习”的基本原理。</p>

<h4 id="使用20中的v1兼容包来沿用1x代码">使用2.0中的v1兼容包来沿用1.x代码</h4>
<p>TensorFlow 2.0中提供了tensorflow.compat.v1代码包来兼容原有1.x的代码，可以做到几乎不加修改的运行。社区的contrib库因为涉及大量直接的TensorFlow引用代码或者自己写的Python扩展包，所以无法使用这种模式。TensorFlow 2.0中也已经移除了contrib库，这让人很有点小遗憾的。<br />
使用这种方式升级原有代码，只需要把原有程序开始的TensorFlow引用:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
</code></pre></div></div>
<p>替换为以下两行就可以正常的继续使用：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow.compat.v1</span> <span class="k">as</span> <span class="n">tf</span>
<span class="n">tf</span><span class="p">.</span><span class="n">disable_v2_behavior</span><span class="p">()</span>
</code></pre></div></div>
<p>其它代码无需修改。个人觉得，如果是稳定使用中、并且没有重构意愿的代码，这种方式算的上首选。</p>

<h4 id="使用迁移工具来自动迁移1x代码到20">使用迁移工具来自动迁移1.x代码到2.0</h4>
<p>TensorFlow 2.0中提供了命令行迁移工具，来自动的把1.x的代码转换为2.0的代码。工具使用方法如下(假设我们的程序文件名称为first-tf.py)：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">$</span> <span class="n">tf_upgrade_v2</span> <span class="o">--</span><span class="n">infile</span> <span class="n">first</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">outfile</span> <span class="n">first</span><span class="o">-</span><span class="n">tf</span><span class="o">-</span><span class="n">v2</span><span class="p">.</span><span class="n">py</span>
</code></pre></div></div>
<p>迁移工具还可以对整个文件夹的程序做升级，请参考工具自身的帮助文档。<br />
使用迁移工具升级的代码，实质上也是使用了tensorflow.compat.v1兼容包来提供在TensorFlow 2.0环境中执行1.x的代码。这里贴出自动转换后的新代码供你对比参考：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#引入TensorFlow库
</span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="c1">#import tensorflow.compat.v1 as tf
</span><span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">disable_v2_behavior</span><span class="p">()</span>

<span class="c1">#引入数值计算库
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#使用 NumPy 生成假数据集x,代表房间的平米数，这里的取值范围是0-1的浮点数，
#原因请看正文中的说明，属于是“规范化”之后的数据
# 生成的数据共100个，式样是100行，每行1个数据
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1">#我们假设每平米0.5万元，基础费用0.7万，这个数值也是规范化之后的，仅供示例
#最终运行的结果，应当求出来0.5/0.7这两个值代表计算成功
#计算最终房价y，x和y一同当做我们的样本数据
# np.dot的意思就是向量x * 0.5
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span>
<span class="c1">#---------------------------------数据集准备完成
#以下使用TensorFlow构建数学模型，在这个过程中，
#直到调用.run之前，实际上都是构造模型，而没有真正的运行。
#这跟上面的numpy库每一次都是真正执行是截然不同的区别
# 请参考正文，我们假定房价的公式为：y=a*x+b
</span>
<span class="c1">#tf.Variable是在TensorFlow中定义一个变量的意思
#我们这里简单起见，人为给a/b两个初始值，都是0.3，注意这也是相当于规范化之后的数值
</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="c1">#这是定义主要的数学模型，模型来自于上面的公式
#注意这里必须使用tf的公式，这样的公式才是模型
#上面使用np的是直接计算，而不是定义模型
# TensorFlow的函数名基本就是完整英文，你应当能读懂
</span><span class="n">y_value</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># 这里是代价函数，同我们文中所讲的唯一区别是用平方来取代求绝对值，
#目标都是为了得到一个正数值，功能完全相同，
#平方计算起来会更快更容易,这种方式也称为“方差“
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_value</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
<span class="c1"># TensorFlow内置的梯度下降算法，每步长0.5
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># 代价函数值最小化的时候，代表求得解
</span><span class="n">train</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1"># 初始化所有变量，也就是上面定义的a/b两个变量
</span><span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="c1">#启动图
</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span>
<span class="c1">#真正的执行初始化变量，还是老话，上面只是定义模型，并没有真正开始执行
</span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

<span class="c1">#重复梯度下降200次，每隔5次打印一次结果
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
    <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>	
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>

</code></pre></div></div>
<p>转换之后，代码中的注释部分会完美的保留，喜欢用代码来代替文档的程序员可以放心。所有2.0中变更了的类或者方法，转换工具将使用tensorflow.compat.v1中的对应类或方法来替代，比如：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</code></pre></div></div>
<p>所以从本质上，这种方式跟第一种方法，采用tensorflow.compat.v1包作为tensorflow替代包的方式是完全相同的。</p>

<h4 id="编写原生的tensorflow-20程序">编写原生的TensorFlow 2.0程序</h4>
<p>推荐的演进方式，当然还是学习TensorFlow 2.0的相关特征，重构原有代码为新版本代码才是正路。平心而论，毕竟绝大多数系统的升级都是为了提供更多功能和降低使用门槛。TensorFlow 2.0也是大幅的降低了使用门槛的。大多数的工作比起1.x版本来，都能使用更少的代码量来完成。<br />
首先了解一下TensorFlow 2.0同1.x之间的重要区别：</p>
<ul>
  <li>在API层面的类、方法有了较大的变化，这个需要在使用中慢慢熟悉</li>
  <li>取消了Session机制，每一条命令直接执行(Eager execution)，而不需要等到Session.run</li>
  <li>因为取消了Session机制，原有的数学模型定义，改为使用Python函数编写。原来的feed_dict和tf.placeholder，成为了函数的输入部分；原来的fetches，则成为了函数的返回值。</li>
  <li>使用keras的模型体系对原有的TensorFlow API进行高度的抽象，使用更容易</li>
  <li>使用tf.keras.Model.fit来替代原有的训练循环。</li>
</ul>

<p>正常情况下，最后一项tf.keras.Model.fit能够大大的降低训练循环的代码量。但在本例中，我们模拟了一个现实中并不适用的例子，keras中并未对这种情形进行优化。所以在本例中反而无法使用tf.keras.Model.fit（实际上一定要使用也是可以的，不过要自定义模型，工作量更不划算）。因此本例中仍然要自己编写训练循环。并且因为2.0中API的变化，代码更复杂了。不过相信我，等到比较正式应用中，使用神经网络、卷积等常用算法，代码就极大的简化了。<br />
使用TensorFlow 2.0原生代码的程序代码如下：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/usr/bin/env python3
#上面一行改为使用python3解释本代码
</span>
<span class="c1">#引入python新版本的语言特征
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>

<span class="c1">#引入TensorFlow库,版本2.0
</span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1">#引入数值计算库
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#使用 NumPy 生成假数据集x,代表房间的平米数，这里的取值范围是0-1的浮点数，
#原因请看正文中的说明，属于是“规范化”之后的数据
# 生成的数据共100个，式样是100行，每行1个数据
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1">#我们假设每平米0.5万元，基础费用0.7万，这个数值也是规范化之后的，仅供示例
#最终运行的结果，应当求出来0.5/0.7这两个值代表计算成功
#计算最终房价y，x和y一同当做我们的样本数据
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.7</span>
<span class="c1">#---------------------------------数据集准备完成
# 请参考正文，我们假定房价的公式为：y=a*x+b
#定义tensorflow变量，a是权重，b是偏移
</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="c1">#以上代码基本同tensorflow1.x版本一致
#以下有了区别
#使用python语言定义数学模型，模型来自于上面的公式
#上面使用np的是直接计算得到训练样本，而不是定义模型
#模型中并非必须使用tensorflow的计算函数来代替python的乘法运算
</span><span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span>

<span class="c1">#定义代价函数，也是python函数
</span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">predicted_y</span><span class="p">,</span> <span class="n">desired_y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">predicted_y</span> <span class="o">-</span> <span class="n">desired_y</span><span class="p">))</span>

<span class="c1"># TensorFlow内置Adam算法，每步长0.1
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># 还可以选用TensorFlow内置SGD(随机最速下降)算法，每步长0.001
#不同算法要使用适当的步长，步长过大会导致模型无法收敛
#optimizer = tf.optimizers.SGD(0.001)
</span>
<span class="c1">#重复梯度下降200次，每隔5次打印一次结果
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>	<span class="c1">#进行一次计算
</span>                <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>	<span class="c1">#得到当前损失值
</span>                <span class="n">grads</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">current_loss</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>	<span class="c1">#调整模型中的权重、偏移值
</span>                <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">]))</span>	<span class="c1">#调整之后的值代回到模型
</span>        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>	<span class="c1">#每5次迭代显示一次结果
</span>                <span class="k">print</span><span class="p">(</span> <span class="s">"Step:%d loss:%%%2.5f weight:%2.7f bias:%2.7f "</span> <span class="o">%</span> 
                        <span class="p">(</span><span class="n">step</span><span class="p">,</span><span class="n">current_loss</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">a</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="n">numpy</span><span class="p">()))</span>
</code></pre></div></div>
<p>程序在升级中所做的修改和特殊的处理，都使用注释保留在了源码中。我觉得这种方式比打散摘出来讲解的能更透彻。<br />
最后看一下新版程序的执行结果：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Step</span><span class="p">:</span><span class="mi">0</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">25.78244</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4000000</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.4000000</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">5</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">2.71975</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.7611420</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7740188</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">10</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">3.09600</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.6725605</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7224629</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">15</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.87834</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4931822</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.5800986</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">20</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">1.24737</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4960071</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6186275</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">25</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.22444</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5730734</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7264798</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">30</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.47145</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5464076</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7252067</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">35</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.09156</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4736322</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6712209</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">40</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.14845</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4771673</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6866464</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">45</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.06199</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5101752</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7255269</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">50</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.03108</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4946054</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7112849</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">55</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.04115</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4770990</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6918764</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">60</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00145</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4950625</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7060429</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">65</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.01781</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5029647</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7096580</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">70</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00211</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4934593</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6963260</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">75</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00298</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4983235</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6982682</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">80</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00345</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5049748</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7031375</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">85</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00004</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5001755</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6976562</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">90</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00102</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5002422</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6978318</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">95</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00065</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5029225</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7010939</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">100</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00001</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5000774</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6990223</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">105</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00021</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4996552</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6993059</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">110</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00015</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5007215</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7008768</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">115</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4993480</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6997767</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">120</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00003</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4995552</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7000407</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">125</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00004</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5001000</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7004969</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">130</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00001</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4995880</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6998325</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">135</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4999941</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7000810</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">140</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00001</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5001197</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7000892</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">145</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4999250</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6998329</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">150</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5001498</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7000451</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">155</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5000388</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6999565</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">160</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4999948</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6999494</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">165</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5000526</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7000424</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">170</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4999576</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6999717</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">175</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4999971</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7000214</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">180</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4999900</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7000131</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">185</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4999775</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6999928</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">190</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.5000094</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.7000152</span> 
<span class="n">Step</span><span class="p">:</span><span class="mi">195</span> <span class="n">loss</span><span class="p">:</span><span class="o">%</span><span class="mf">0.00000</span> <span class="n">weight</span><span class="p">:</span><span class="mf">0.4999923</span> <span class="n">bias</span><span class="p">:</span><span class="mf">0.6999906</span> 
</code></pre></div></div>
<p>模型通过学习后，得到的结果是很接近我们的预设值的。<br />
程序中还可以考虑使用随机快速下降算法(SGD)，你可以把当前的Adam算法使用注释符屏蔽上，打开SGD算法的注释屏蔽来尝试一下。对于本例中的数据集来讲，SGD的下降步长需要的更小，同样循环次数下，求解的精度会低一些。可以看出对于本例，Adam算法显然是更有效率的。而在TensorFlow的支持下，对于同样的数据集和数学模型，变更学习算法会很容易。</p>

<p>（待续…）</p>

:ET